{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import hist.dask\n",
    "\n",
    "# this avoids some warnings due to using older NanoAOD ntuples\n",
    "NanoAODSchema.warn_missing_crossrefs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below introduces some basic concepts for writing code using Coffea.  \n",
    "\n",
    "There are three primary pieces to the Coffea code:  \n",
    "1. The *processor*, which contains all of the analysis cuts and fills the histogram in the _process_ function. \n",
    "2. The second cell defines the files we want to run over and then runs the code using run_uproot_job. \n",
    "3. After we run the processor, we can then plot any of the histograms we have generated.\n",
    "\n",
    "To test any changes you make to the histograms, you will have to rerun each of the three cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = {\n",
    "    \"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTGamma_1l.root\": \"Events\"\n",
    "}\n",
    "events = NanoEventsFactory.from_root(\n",
    "    path,\n",
    "    schemaclass=NanoAODSchema,\n",
    "    metadata={\"dataset\": \"TTGamma\"},\n",
    ").events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Muon.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TTGammaCutflow(events):\n",
    "    ## To access variables from the ntuples, use the \"events\" object\n",
    "    ## The dataset name is part of events.metadata\n",
    "    dataset = events.metadata['dataset']\n",
    "    \n",
    "    ## The coffea NanoEventSchema packages all muon variables (columns) into the events.Muon object\n",
    "    ## Each variable can be accessed using muons.key_name\n",
    "    muons = events.Muon\n",
    "    electrons = events.Electron\n",
    "    \n",
    "    ######\n",
    "    \n",
    "    # Select muons with pt >30, eta < 2.4, tight ID, and relIso < 0.15\n",
    "    muonSelectTight = (\n",
    "        (muons.pt > 30)\n",
    "        & (abs(muons.eta) < 2.4)\n",
    "        & muons.tightId\n",
    "    )\n",
    "\n",
    "    # Apply the selection to muons using the array[mask] syntax. \n",
    "    # tightMuons only includes the muons that pass the tight selection we defined\n",
    "    tightMuons = muons[muonSelectTight]\n",
    "    \n",
    "    ######\n",
    "    ### Step 2. Define your own selection for electrons. \n",
    "    ### Note that the ID variable names will be different. \n",
    "    ### Either remove the ID and iso variables or replace them with the correct electron values.\n",
    "    ######\n",
    "    \n",
    "    eleSelectTight = (\n",
    "        electrons.pt > 30\n",
    "    )\n",
    "    tightElectrons = electrons[eleSelectTight]\n",
    "\n",
    "    ######\n",
    "    \n",
    "    # Select events with exactly one tight muon. \n",
    "    eventSelectionMuon = (ak.num(tightMuons)==1)\n",
    "    \n",
    "    ######\n",
    "    ### Step 3. Define a second event selection requiring events with no muons and exactly one electron\n",
    "    ###### \n",
    "    eventSelectionEle = ()\n",
    "\n",
    "    ######\n",
    "    # we will make a few different histograms, and store them in this dictionary\n",
    "    output = {}\n",
    "\n",
    "    # Define and fill a muon_pt histogram using the tightMuons in events that pass our selection \n",
    "    # Note that ak.flatten() is required when filling a histogram to remove the jaggedness\n",
    "    output['muon_pt'] = hist.dask.Hist(\n",
    "        hist.axis.Regular(40, 0, 200, name=\"pt\", label=\"Muon $p_{T}$ [GeV]\")\n",
    "    )\n",
    "    output['muon_pt'].fill(pt=ak.flatten(tightMuons[eventSelectionMuon].pt))\n",
    "    \n",
    "    ######\n",
    "    ### Step 4. Fill an ele_pt histogram\n",
    "    ###### \n",
    "\n",
    "    \n",
    "    ######\n",
    "    \n",
    "    \n",
    "    # we can return any nested dictionary of histograms or numbers\n",
    "    # by having one set of histograms per dataset we can properly scale\n",
    "    # the simulation by its cross sections later\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    output, = dask.compute(TTGammaCutflow(events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import dataset_tools\n",
    "\n",
    "# Define files to run over\n",
    "samples = {\n",
    "    \"TTGamma\": {\n",
    "        \"files\": {\n",
    "            \"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTGamma_1l.root\": \"Events\",\n",
    "        },\n",
    "    },\n",
    "    \"TTbar\": {\n",
    "        \"files\": {\n",
    "            \"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTbar_1l.root\": \"Events\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "samples, _ = dataset_tools.preprocess(samples, maybe_step_size=40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = dataset_tools.apply_to_fileset(\n",
    "    TTGammaCutflow, samples, uproot_options={\"allow_read_errors_with_report\": True}\n",
    ")\n",
    "\n",
    "with ProgressBar():\n",
    "    output, report = dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_awkward as dak\n",
    "\n",
    "dak.necessary_columns(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    name: TTGammaCutflow(\n",
    "        NanoEventsFactory.from_root(\n",
    "            spec[\"files\"],\n",
    "            schemaclass=NanoAODSchema,\n",
    "            metadata={\"dataset\": name},\n",
    "        ).events()\n",
    "    )\n",
    "    for name, spec in samples.items()\n",
    "}\n",
    "with ProgressBar():\n",
    "    output, = dask.compute(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttgamma.utils.crossSections import lumis, crossSections\n",
    "import uproot\n",
    "\n",
    "myCrossSections = {}\n",
    "myCrossSections['TTGamma'] = crossSections['TTGamma_SingleLept']\n",
    "myCrossSections['TTbar'] = crossSections['TTbarPowheg_Semilept']\n",
    "\n",
    "for dataset_name, histos in output.items():\n",
    "    histos[\"InputEventCount\"] = 0\n",
    "    for path in fileset[dataset_name]:\n",
    "        filename = list(path)[0]\n",
    "        with uproot.open(filename) as fhandle:\n",
    "            histos[\"InputEventCount\"] += (\n",
    "            fhandle[\"hEvents\"].values()[2] - fhandle[\"hEvents\"].values()[0]\n",
    "        )\n",
    "                    \n",
    "    for name, obj in histos.items():\n",
    "        if isinstance(obj, hist.Hist):\n",
    "            obj *= myCrossSections[dataset_name] * lumis[2016] / histos[\"InputEventCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(5, 8))\n",
    "# Plot the muon_pt histogram, overlaying the distribution for each dataset\n",
    "# Sometimes have to run this cell twice for the plot to appear\n",
    "output['TTGamma']['muon_pt'].plot1d(ax=ax1, label='ttgamma');\n",
    "output['TTbar']['muon_pt'].plot1d(ax=ax1, label='ttbar');\n",
    "ax1.legend();\n",
    "\n",
    "######\n",
    "### Step 5. Plot the ele_pt histogram\n",
    "###### \n",
    "\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to look at other files, feel free to edit _fileset_ above using the skimmed datasets here: /store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['TTGamma']['muon_pt'].plot1d(density=True, label='ttgamma');\n",
    "output['TTbar']['muon_pt'].plot1d(density=True, label='ttbar');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the electron pT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've seen some of the basics of histogramming, object cuts, and event selection, let's go through how to apply weights to the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from coffea.lookup_tools import dense_lookup\n",
    "from coffea.analysis_tools import Weights\n",
    "\n",
    "class TTGammaWeights:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # There are several utilities within coffea that can help us apply weights and systematics to the code\n",
    "        # We use uproot to open the root file containing the electron ID scale factors:\n",
    "        ele_id_file = uproot.open('ttgamma/scalefactors/MuEGammaScaleFactors/ele2016/2016LegacyReReco_ElectronTight_Fall17V2.root')\n",
    "        # The dense_lookup tools in Coffea make it easy to extract the histogram (named EGamma_SF2D in this case)\n",
    "        # with the weights and their errors:\n",
    "        self.ele_id_sf = dense_lookup.dense_lookup(\n",
    "            ele_id_file[\"EGamma_SF2D\"].values(),\n",
    "            (\n",
    "                ele_id_file[\"EGamma_SF2D\"].axis(0).edges(),\n",
    "                ele_id_file[\"EGamma_SF2D\"].axis(1).edges()\n",
    "            )\n",
    "        )\n",
    "        self.ele_id_err = dense_lookup.dense_lookup(\n",
    "            ele_id_file[\"EGamma_SF2D\"].variances()**0.5,\n",
    "            (\n",
    "                ele_id_file[\"EGamma_SF2D\"].axis(0).edges(),\n",
    "                ele_id_file[\"EGamma_SF2D\"].axis(1).edges()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # We can also pre-define our histograms as shown below, but each process() call\n",
    "        # needs to create a \"fresh\" copy of the histogram or else you will count events multiple times\n",
    "        # when the outputs are merged by the coffea tools. That's why we define the `make_output` factory function\n",
    "\n",
    "        # The systematic axis will be used to keep track of whether we are using the nominal weights\n",
    "        # or the weights shifted up/down by their errors\n",
    "        systematic_axis = hist.axis.StrCategory([], name=\"systematic\", label=\"Systematic uncertainty\", growth=True)\n",
    "        \n",
    "        ele_pt_axis  = hist.axis.Regular(40, 0, 200, name=\"pt\", label=\"Electron $p_{T}$ [GeV]\")\n",
    "\n",
    "        self.make_output = lambda: {\n",
    "            'ele_pt' : hist.dask.Hist(systematic_axis, ele_pt_axis),\n",
    "        }\n",
    "\n",
    "    def process(self, events):\n",
    "\n",
    "        output = self.make_output()\n",
    "\n",
    "        dataset = events.metadata['dataset']\n",
    "\n",
    "        electrons = events.Electron\n",
    "        \n",
    "        #Define tight electron selection\n",
    "        electronSelectTight = (\n",
    "            (electrons.pt>35) & \n",
    "            (abs(electrons.eta)<2.1) & \n",
    "            ((abs(electrons.eta) < 1.4442) | (abs(electrons.eta) > 1.566)) &      \n",
    "            (electrons.cutBased>=4)\n",
    "        )\n",
    "        # Apply selection\n",
    "        tightElectron = electrons[electronSelectTight]\n",
    "        #Define event selection\n",
    "        eventSelection = (ak.num(tightElectron) == 1)\n",
    "\n",
    "\n",
    "        # Here, we look up the scale factors and errors for each electron using the electron eta and pt\n",
    "        eleID = self.ele_id_sf(tightElectron.eta, tightElectron.pt)\n",
    "        eleIDerr = self.ele_id_err(tightElectron.eta, tightElectron.pt)\n",
    "        \n",
    "        #To get an event-level weight, multiply the SF for each electron in the event\n",
    "            # SF : scale factor\n",
    "        #The axis=-1 option means we are multiplying the innermost values (electrons per event in this case)\n",
    "        eleSF = ak.prod(eleID, axis=-1) # product of elements along axis\n",
    "        eleSFUp = ak.prod(eleID+eleIDerr, axis=-1)\n",
    "        eleSFDown = ak.prod(eleID-eleIDerr, axis=-1)\n",
    "        print(eleSF, eleSFUp, eleSFDown)\n",
    "    \n",
    "        # The Weights object is a container that handles the bookkeeping \n",
    "        # for event weights and associated systematic shifts.\n",
    "        # The argument for the Weights object is the number of events we are processing\n",
    "        weights = Weights(None)\n",
    "\n",
    "        # Add the ele ID SF to the weights object\n",
    "        weights.add('eleEffWeight',weight=eleSF,weightUp=eleSFUp,weightDown=eleSFDown)\n",
    "        \n",
    "        systList = ['noweight','nominal','eleEffWeightUp','eleEffWeightDown']\n",
    "\n",
    "        for syst in systList:\n",
    "           \n",
    "            weightSyst = syst\n",
    "            if syst=='nominal':\n",
    "                weightSyst=None\n",
    "                \n",
    "            if syst=='noweight':\n",
    "                evtWeight = ak.ones_like(events.MET.pt)\n",
    "            else:\n",
    "                evtWeight = weights.weight(weightSyst) # overall event weight\n",
    "                \n",
    "            output['ele_pt'].fill(\n",
    "                systematic=syst,\n",
    "                pt=ak.flatten(tightElectron[eventSelection].pt),\n",
    "                weight=evtWeight[eventSelection]\n",
    "            )\n",
    "           \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from coffea import dataset_tools\n",
    "\n",
    "# Define files to run over\n",
    "fileset = {\n",
    "    \"TTGamma\": [\n",
    "        {\"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTGamma_1l.root\": \"Events\"},\n",
    "    ],\n",
    "    \"TTbar\": [\n",
    "        {\"root://cmseos.fnal.gov//store/user/cmsdas/2021/long_exercises/TTGamma/TestFiles/TTbar_1l.root\": \"Events\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "analysis = TTGammaWeights()\n",
    "output = {\n",
    "    name: analysis.process(\n",
    "        NanoEventsFactory.from_root(\n",
    "            path,\n",
    "            schemaclass=NanoAODSchema,\n",
    "            metadata={\"dataset\": name},\n",
    "            permit_dask=True,\n",
    "        ).events()\n",
    "    )\n",
    "    for name, path in fileset.items()\n",
    "}\n",
    "with ProgressBar():\n",
    "    output, = dask.compute(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pt distributions for each of the different systematics\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "output['TTGamma']['ele_pt'].plot1d(overlay='systematic');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.prod([[1,2,3],[4,5]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ak.prod([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttgamma.scalefactors as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.ele_id_sf # same as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TTGammaWeights().ele_id_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
